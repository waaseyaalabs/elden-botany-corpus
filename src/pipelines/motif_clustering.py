"""Motif clustering pipeline powering the Phase 7 analysis layer."""

from __future__ import annotations

import json
import logging
import math
from dataclasses import dataclass
from pathlib import Path
from typing import Any, cast

import click
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from corpus.community_schema import MotifTaxonomy, load_motif_taxonomy
from corpus.config import settings
from numpy.typing import NDArray

from pipelines.embedding_backends import (
    EncoderConfig,
    ProviderLiteral,
    create_encoder,
)
from pipelines.motif_coverage import COVERAGE_FILENAME
from pipelines.motif_taxonomy_utils import (
    compile_motif_patterns,
    detect_motif_hits,
)

LOGGER = logging.getLogger(__name__)


Matrix = NDArray[np.float32]
Labels = NDArray[np.int64]
Probabilities = NDArray[np.float64]


@dataclass(slots=True)
class ProjectionResult:
    """Container for high-dimensional and 2D UMAP projections."""

    cluster_components: Matrix
    viz_components: Matrix


@dataclass(slots=True)
class MotifClusteringConfig:
    """Runtime configuration for motif clustering."""

    curated_path: Path | None = None
    coverage_path: Path | None = None
    output_dir: Path = Path("data/analysis")
    embedding_provider: ProviderLiteral = "local"
    embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"
    embedding_batch_size: int = 64
    random_seed: int = 42
    max_rows: int | None = None
    min_cluster_size: int = 10
    min_samples: int | None = None
    umap_neighbors: int = 15
    umap_min_dist: float = 0.1
    umap_components: int = 15
    exemplars_per_cluster: int = 5


@dataclass(slots=True)
class MotifClusterArtifacts:
    """Paths to downstream artifacts generated by the pipeline."""

    cluster_parquet: Path
    samples_json: Path
    umap_plot: Path
    motif_density_parquet: Path


class MotifClusteringPipeline:
    """Orchestrates embedding, dimensionality reduction, and clustering."""

    def __init__(
        self,
        config: MotifClusteringConfig | None = None,
        taxonomy: MotifTaxonomy | None = None,
    ) -> None:
        self.config = config or MotifClusteringConfig()
        self._taxonomy = taxonomy or load_motif_taxonomy()
        self._rng = np.random.default_rng(self.config.random_seed)
        self._motif_patterns = compile_motif_patterns(self._taxonomy)

    def run(self) -> MotifClusterArtifacts:
        """Execute the clustering workflow and persist artifacts."""

        lore_frame = self._load_lore()
        if lore_frame.empty:
            raise RuntimeError(
                "Curated lore frame is empty; nothing to cluster"
            )

        embeddings = self._embed_rows(lore_frame)
        projections = self._project_embeddings(embeddings)
        labels, probabilities = self._cluster_embeddings(
            projections.cluster_components
        )
        enriched = self._attach_projection_columns(
            lore_frame,
            labels,
            probabilities,
            projections.viz_components,
        )
        coverage = self._load_coverage()
        motif_hits = self._detect_motif_hits(enriched["text"].astype(str))
        density_df, summaries = summarize_clusters(
            enriched,
            motif_hits,
            coverage,
            exemplars=self.config.exemplars_per_cluster,
        )
        artifacts = self._write_artifacts(enriched, density_df, summaries)
        return artifacts

    def _load_lore(self) -> pd.DataFrame:
        path = self.config.curated_path or (
            settings.curated_dir / "lore_corpus.parquet"
        )
        fallback = path.with_suffix(".csv")
        if not path.exists() and not fallback.exists():
            message = (
                "Unable to locate lore corpus artifact. "
                "Run 'make build-corpus' or provide --curated."
            )
            raise FileNotFoundError(message)

        frame: pd.DataFrame
        if path.exists():
            frame = pd.read_parquet(path)
        else:  # pragma: no cover - CSV fallback for edge deployments
            frame = pd.read_csv(fallback)

        required = {"lore_id", "canonical_id", "text", "category", "text_type"}
        missing = required - set(frame.columns)
        if missing:
            formatted = ", ".join(sorted(missing))
            raise ValueError(f"Lore corpus missing columns: {formatted}")

        trimmed = frame.loc[frame["text"].notna()].copy()
        trimmed["text"] = trimmed["text"].astype(str).str.strip()
        trimmed = trimmed.loc[trimmed["text"] != ""]
        trimmed.drop_duplicates(subset=["lore_id"], inplace=True)
        trimmed.reset_index(drop=True, inplace=True)
        sampled = self.sample_frame(trimmed)
        LOGGER.info("Loaded %s lore rows for clustering", len(sampled))
        return sampled

    def sample_frame(self, frame: pd.DataFrame) -> pd.DataFrame:
        if self.config.max_rows is None or len(frame) <= self.config.max_rows:
            return frame

        indices = self._rng.choice(
            len(frame),
            size=self.config.max_rows,
            replace=False,
        )
        sampled = frame.iloc[np.sort(indices)].reset_index(drop=True)
        return sampled

    def _embed_rows(self, frame: pd.DataFrame) -> Matrix:
        texts = frame["text"].astype(str).tolist()
        if not texts:
            raise RuntimeError("No lore text available for embedding")

        encoder_config = EncoderConfig(
            provider=self.config.embedding_provider,
            model_name=self.config.embedding_model,
            batch_size=self.config.embedding_batch_size,
            openai_api_key=settings.openai_api_key or None,
        )
        encoder = create_encoder(encoder_config)
        vectors = encoder.encode(texts)
        array = np.asarray(vectors, dtype=np.float32)
        return array

    def _project_embeddings(self, embeddings: Matrix) -> ProjectionResult:
        try:
            from umap import UMAP
        except ImportError as exc:  # pragma: no cover - dependency guard
            raise ImportError(
                "umap-learn is required for motif clustering. Install with "
                "'poetry add umap-learn'."
            ) from exc

        reducer = UMAP(
            n_neighbors=self.config.umap_neighbors,
            min_dist=self.config.umap_min_dist,
            n_components=self.config.umap_components,
            metric="cosine",
            random_state=self.config.random_seed,
        )
        cluster_components = reducer.fit_transform(embeddings)

        viz_reducer = UMAP(
            n_neighbors=self.config.umap_neighbors,
            min_dist=0.01,
            n_components=2,
            metric="cosine",
            random_state=self.config.random_seed,
        )
        viz_components = viz_reducer.fit_transform(embeddings)
        return ProjectionResult(cluster_components, viz_components)

    def _cluster_embeddings(
        self, components: Matrix
    ) -> tuple[Labels, Probabilities]:
        try:
            import hdbscan
        except ImportError as exc:  # pragma: no cover - dependency guard
            raise ImportError(
                "hdbscan is required for motif clustering. Install with "
                "'poetry add hdbscan'."
            ) from exc

        min_samples = self.config.min_samples or self.config.min_cluster_size
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=self.config.min_cluster_size,
            min_samples=min_samples,
            metric="euclidean",
            cluster_selection_method="eom",
            prediction_data=False,
        )
        labels = clusterer.fit_predict(components)
        raw_probabilities = getattr(
            clusterer,
            "probabilities_",
            np.ones_like(labels, dtype=float),
        )
        labels_array = cast(Labels, np.asarray(labels, dtype=np.int64))
        probabilities = cast(
            Probabilities, np.asarray(raw_probabilities, dtype=float)
        )
        return labels_array, probabilities

    def _attach_projection_columns(
        self,
        frame: pd.DataFrame,
        labels: Labels,
        probabilities: Probabilities,
        viz_components: Matrix,
    ) -> pd.DataFrame:
        enriched = frame.copy()
        enriched["cluster_id"] = labels.astype(int)
        enriched["cluster_probability"] = probabilities.astype(float)
        enriched["umap_x"] = viz_components[:, 0]
        enriched["umap_y"] = viz_components[:, 1]
        return enriched

    def _load_coverage(self) -> pd.DataFrame | None:
        coverage_path = self.config.coverage_path or (
            settings.community_processed_dir / COVERAGE_FILENAME
        )
        if not coverage_path.exists():
            LOGGER.warning(
                "Motif coverage artifact not found at %s", coverage_path
            )
            return None

        frame = pd.read_parquet(coverage_path)
        return frame

    def _detect_motif_hits(self, texts: pd.Series) -> pd.DataFrame:
        return detect_motif_hits(texts, self._motif_patterns)

    def _write_artifacts(
        self,
        enriched: pd.DataFrame,
        density_df: pd.DataFrame,
        summaries: list[dict[str, Any]],
    ) -> MotifClusterArtifacts:
        analysis_dir = self.config.output_dir / "motif_clustering"
        analysis_dir.mkdir(parents=True, exist_ok=True)

        cluster_path = analysis_dir / "motif_clusters.parquet"
        density_path = analysis_dir / "motif_cluster_density.parquet"
        samples_path = analysis_dir / "motif_cluster_samples.json"
        plot_path = analysis_dir / "motif_clusters.png"

        enriched.to_parquet(cluster_path, index=False)
        density_df.to_parquet(density_path, index=False)

        payload = {
            "summary": {
                "cluster_count": int(enriched["cluster_id"].nunique()),
                "rows": len(enriched),
                "taxonomy_version": self._taxonomy.version,
            },
            "clusters": summaries,
        }
        samples_path.write_text(
            json.dumps(payload, indent=2, ensure_ascii=False),
            encoding="utf-8",
        )

        self._render_plot(enriched, plot_path)

        LOGGER.info("Motif clustering artifacts written to %s", analysis_dir)
        return MotifClusterArtifacts(
            cluster_parquet=cluster_path,
            samples_json=samples_path,
            umap_plot=plot_path,
            motif_density_parquet=density_path,
        )

    def _render_plot(self, frame: pd.DataFrame, path: Path) -> None:
        plt.figure(figsize=(10, 8))
        scatter = plt.scatter(
            frame["umap_x"],
            frame["umap_y"],
            c=frame["cluster_id"],
            cmap="tab20",
            s=14,
            alpha=0.8,
        )
        plt.xlabel("UMAP-1")
        plt.ylabel("UMAP-2")
        plt.title("Motif clusters")
        plt.colorbar(scatter, label="cluster id")
        plt.tight_layout()
        plt.savefig(path, dpi=200)
        plt.close()


def summarize_clusters(
    frame: pd.DataFrame,
    motif_hits: pd.DataFrame,
    coverage: pd.DataFrame | None,
    *,
    exemplars: int,
) -> tuple[pd.DataFrame, list[dict[str, Any]]]:
    """Compute motif density stats and cluster exemplars."""

    coverage_lookup = _coverage_lookup(coverage)
    density_df = _build_density_frame(frame, motif_hits, coverage_lookup)
    summaries = _build_cluster_summaries(
        frame,
        density_df,
        coverage_lookup,
        exemplars,
    )
    return density_df, summaries


def _coverage_lookup(
    coverage: pd.DataFrame | None,
) -> dict[str, dict[str, Any]]:
    if coverage is None:
        return {}
    lookup: dict[str, dict[str, Any]] = {}
    for row in coverage.to_dict("records"):
        slug = str(row.get("motif_slug"))
        lookup[slug] = {
            "label": row.get("label"),
            "category": row.get("category"),
            "coverage_pct": row.get("coverage_pct"),
        }
    return lookup


def _build_density_frame(
    frame: pd.DataFrame,
    motif_hits: pd.DataFrame,
    coverage_lookup: dict[str, dict[str, Any]],
) -> pd.DataFrame:
    records: list[dict[str, Any]] = []
    if motif_hits.empty:
        return pd.DataFrame(records)

    clusters = frame["cluster_id"].unique().tolist()
    for cluster_id in sorted(int(value) for value in clusters):
        cluster_mask = frame["cluster_id"] == cluster_id
        cluster_size = int(cluster_mask.sum())
        if cluster_size == 0:
            continue

        for slug in motif_hits.columns:
            hits = int(motif_hits.loc[cluster_mask, slug].sum())
            if hits == 0:
                continue
            cluster_pct = round((hits / cluster_size) * 100, 2)
            coverage_pct = coverage_lookup.get(slug, {}).get("coverage_pct")
            delta_pct = (
                round(cluster_pct - float(coverage_pct), 2)
                if coverage_pct is not None
                else None
            )
            records.append(
                {
                    "cluster_id": cluster_id,
                    "cluster_size": cluster_size,
                    "motif_slug": slug,
                    "motif_label": coverage_lookup.get(slug, {}).get("label"),
                    "motif_category": coverage_lookup.get(slug, {}).get(
                        "category"
                    ),
                    "cluster_match_count": hits,
                    "cluster_pct": cluster_pct,
                    "global_pct": coverage_pct,
                    "delta_pct": delta_pct,
                }
            )

    return pd.DataFrame(records)


def _build_cluster_summaries(
    frame: pd.DataFrame,
    density_df: pd.DataFrame,
    coverage_lookup: dict[str, dict[str, Any]],
    exemplars: int,
) -> list[dict[str, Any]]:
    summaries: list[dict[str, Any]] = []
    clusters = sorted(int(value) for value in frame["cluster_id"].unique())
    for cluster_id in clusters:
        cluster_rows = frame[frame["cluster_id"] == cluster_id]
        cluster_size = int(cluster_rows.shape[0])
        top_motifs = _select_top_motifs(density_df, cluster_id)
        exemplars_payload = select_exemplars(cluster_rows, limit=exemplars)
        summaries.append(
            {
                "cluster_id": cluster_id,
                "size": cluster_size,
                "noise": cluster_id == -1,
                "top_motifs": top_motifs,
                "exemplars": exemplars_payload,
            }
        )
    return summaries


def _coerce_percentage(value: Any) -> float | None:
    if value is None:
        return None
    try:
        numeric = float(value)
    except (TypeError, ValueError):
        return None
    if math.isnan(numeric):
        return None
    return numeric


def _select_top_motifs(
    density_df: pd.DataFrame, cluster_id: int
) -> list[dict[str, Any]]:
    if density_df.empty:
        return []
    subset = density_df.loc[density_df["cluster_id"] == cluster_id]
    if subset.empty:
        return []
    ranked = subset.sort_values(
        by=["cluster_pct", "cluster_match_count"],
        ascending=[False, False],
    ).head(5)
    return [
        {
            "motif_slug": str(row["motif_slug"]),
            "motif_label": row.get("motif_label"),
            "motif_category": row.get("motif_category"),
            "cluster_pct": float(row["cluster_pct"]),
            "global_pct": _coerce_percentage(row.get("global_pct")),
            "delta_pct": row.get("delta_pct"),
            "cluster_match_count": int(row["cluster_match_count"]),
        }
        for _, row in ranked.iterrows()
    ]


def select_exemplars(frame: pd.DataFrame, limit: int) -> list[dict[str, Any]]:
    """Return the highest-confidence exemplars for a cluster."""

    if frame.empty:
        return []

    resolved = frame.sort_values(
        by=["cluster_probability", "text"],
        ascending=[False, True],
    ).head(limit)

    exemplars = [
        {
            "lore_id": str(row["lore_id"]),
            "canonical_id": str(row["canonical_id"]),
            "text": str(row["text"]),
            "category": row.get("category"),
            "text_type": row.get("text_type"),
            "probability": float(row.get("cluster_probability", 0.0)),
        }
        for _, row in resolved.iterrows()
    ]
    return exemplars


@click.command()
@click.option(
    "--curated",
    type=click.Path(dir_okay=False, path_type=Path),
    default=None,
    help=(
        "Optional path to curated lore parquet (defaults to "
        "data/curated/lore_corpus.parquet)."
    ),
)
@click.option(
    "--coverage",
    type=click.Path(dir_okay=False, path_type=Path),
    default=None,
    help="Optional override for motif_coverage.parquet input.",
)
@click.option("--export-dir", type=click.Path(path_type=Path), default=None)
@click.option("--model", type=str, default=None)
@click.option(
    "--provider",
    type=click.Choice(["local", "openai"]),
    default=None,
)
@click.option("--seed", type=int, default=None)
@click.option("--max-rows", type=int, default=None)
@click.option("--min-cluster", type=int, default=None)
@click.option("--min-samples", type=int, default=None)
@click.option("--neighbors", type=int, default=None)
@click.option("--min-dist", type=float, default=None)
@click.option("--components", type=int, default=None)
@click.option("--exemplars", type=int, default=None)
def main(
    curated: Path | None,
    coverage: Path | None,
    export_dir: Path | None,
    model: str | None,
    provider: str | None,
    seed: int | None,
    max_rows: int | None,
    min_cluster: int | None,
    min_samples: int | None,
    neighbors: int | None,
    min_dist: float | None,
    components: int | None,
    exemplars: int | None,
) -> None:
    """CLI entry point for motif clustering (standalone use)."""

    defaults = MotifClusteringConfig()
    provider_value = cast(
        ProviderLiteral, provider or defaults.embedding_provider
    )
    config = MotifClusteringConfig(
        curated_path=curated,
        coverage_path=coverage,
        output_dir=export_dir or defaults.output_dir,
        embedding_model=model or defaults.embedding_model,
        embedding_provider=provider_value,
        random_seed=seed or defaults.random_seed,
        max_rows=max_rows or defaults.max_rows,
        min_cluster_size=min_cluster or defaults.min_cluster_size,
        min_samples=min_samples or defaults.min_samples,
        umap_neighbors=neighbors or defaults.umap_neighbors,
        umap_min_dist=min_dist or defaults.umap_min_dist,
        umap_components=components or defaults.umap_components,
        exemplars_per_cluster=exemplars or defaults.exemplars_per_cluster,
    )

    pipeline = MotifClusteringPipeline(config)
    try:
        artifacts = pipeline.run()
    except (ValueError, FileNotFoundError, RuntimeError) as exc:
        raise click.ClickException(str(exc)) from exc

    click.echo("âœ“ Generated motif clustering artifacts")
    click.echo(f"  Clusters: {artifacts.cluster_parquet}")
    click.echo(f"  Density : {artifacts.motif_density_parquet}")
    click.echo(f"  Samples : {artifacts.samples_json}")
    click.echo(f"  Plot    : {artifacts.umap_plot}")


if __name__ == "__main__":  # pragma: no cover
    main()
